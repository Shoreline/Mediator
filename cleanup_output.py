#!/usr/bin/env python3
"""
æ¸…ç† output/ ç›®å½•ä¸­ä»»åŠ¡æ•°å°äºé˜ˆå€¼çš„æ–‡ä»¶

ä½¿ç”¨æ–¹æ³•ï¼š
    # é¢„è§ˆå°†è¦åˆ é™¤çš„æ–‡ä»¶ï¼ˆä¸å®é™…åˆ é™¤ï¼‰
    python cleanup_output.py --dry-run
    
    # æ¸…ç†ä»»åŠ¡æ•° < 100 çš„æ–‡ä»¶ï¼ˆé»˜è®¤ï¼‰
    python cleanup_output.py
    
    # æ¸…ç†ä»»åŠ¡æ•° < 50 çš„æ–‡ä»¶
    python cleanup_output.py --threshold 50
    
    # è‡ªåŠ¨ç¡®è®¤åˆ é™¤ï¼ˆä¸éœ€è¦äº¤äº’ï¼‰
    python cleanup_output.py --yes
"""

import os
import re
import glob
import argparse
from collections import defaultdict
from typing import List, Dict, Tuple


def parse_jsonl_filename(filename: str) -> Tuple[int, int, str]:
    """
    ä» JSONL æ–‡ä»¶åä¸­æå–ä¿¡æ¯
    
    æ”¯æŒçš„æ ¼å¼ï¼š
    1. æ— é‡‡æ ·: {job_num}_tasks_{total}_{rest}.jsonl
    2. æœ‰é‡‡æ ·: {job_num}_sampled_{rate}_seed{seed}_tasks_{total}_{rest}.jsonl
    
    Returns:
        (job_num, task_count, timestamp_and_model) æˆ– (None, None, None) å¦‚æœæ— æ³•è§£æ
    """
    # å°è¯•åŒ¹é…æœ‰é‡‡æ ·çš„æ ¼å¼
    pattern1 = r'^(\d+)_sampled_[\d.]+_seed\d+_tasks_(\d+)_(.+)\.jsonl$'
    match = re.match(pattern1, filename)
    if match:
        job_num = int(match.group(1))
        task_count = int(match.group(2))
        rest = match.group(3)
        return job_num, task_count, rest
    
    # å°è¯•åŒ¹é…æ— é‡‡æ ·çš„æ ¼å¼
    pattern2 = r'^(\d+)_tasks_(\d+)_(.+)\.jsonl$'
    match = re.match(pattern2, filename)
    if match:
        job_num = int(match.group(1))
        task_count = int(match.group(2))
        rest = match.group(3)
        return job_num, task_count, rest
    
    return None, None, None


def find_related_files(jsonl_filename: str, output_dir: str = 'output') -> List[str]:
    """
    æ ¹æ® JSONL æ–‡ä»¶åæŸ¥æ‰¾æ‰€æœ‰ç›¸å…³æ–‡ä»¶
    
    ç›¸å…³æ–‡ä»¶åŒ…æ‹¬ï¼š
    1. åŸå§‹ JSONL æ–‡ä»¶
    2. CSV è¯„ä¼°æ–‡ä»¶
    3. eval_debug.jsonl æ–‡ä»¶
    4. VSP/CoMT-VSP è¯¦ç»†è¾“å‡ºç›®å½•
    5. é‡‡æ ·ç‰ˆæœ¬çš„ CSV æ–‡ä»¶
    
    Returns:
        ç›¸å…³æ–‡ä»¶å’Œç›®å½•çš„è·¯å¾„åˆ—è¡¨
    """
    related = []
    
    # 1. åŸå§‹ JSONL æ–‡ä»¶
    jsonl_path = os.path.join(output_dir, jsonl_filename)
    if os.path.exists(jsonl_path):
        related.append(jsonl_path)
    
    # æå–åŸºç¡€ä¿¡æ¯
    basename = jsonl_filename.replace('.jsonl', '')
    
    # 2. eval_debug.jsonl æ–‡ä»¶
    eval_debug_path = os.path.join(output_dir, f"{basename}_eval_debug.jsonl")
    if os.path.exists(eval_debug_path):
        related.append(eval_debug_path)
    
    # 3. CSV æ–‡ä»¶ï¼ˆå¯èƒ½æœ‰å¤šç§æ ¼å¼ï¼‰
    # æ ¼å¼1: {job_num}_eval_tasks_{total}_{rest}.csv
    # æ ¼å¼2: {job_num}_eval-sampled_{rate}_seed{seed}_tasks_{total}_{rest}.csv
    
    # è§£æ JSONL æ–‡ä»¶åä»¥æ„å»º CSV æ–‡ä»¶å
    job_num, task_count, rest = parse_jsonl_filename(jsonl_filename)
    
    if job_num is not None:
        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„ CSV æ–‡ä»¶ï¼ˆåŒ…æ‹¬é‡‡æ ·å’Œéé‡‡æ ·ç‰ˆæœ¬ï¼‰
        csv_patterns = [
            f"{job_num}_eval_tasks_{task_count}_*.csv",
            f"{job_num}_eval-sampled_*_tasks_{task_count}_*.csv",
            f"eval_{job_num}_*_tasks_{task_count}_*.csv",  # æ–°æ ¼å¼
        ]
        
        for pattern in csv_patterns:
            csv_files = glob.glob(os.path.join(output_dir, pattern))
            related.extend(csv_files)
    
    # 4. VSP/CoMT-VSP è¯¦ç»†è¾“å‡ºç›®å½•
    # ä»æ–‡ä»¶åä¸­æå– timestamp
    timestamp_match = re.search(r'(\d{4}-\d{2}-\d{2}_\d{2}-\d{2}-\d{2})', basename)
    if timestamp_match:
        timestamp = timestamp_match.group(1)
        
        # VSP ç›®å½•æ ¼å¼
        vsp_dirs = [
            f"{output_dir}/vsp_details/vsp_{timestamp}",
            f"{output_dir}/vsp_details/{job_num}_tasks_{task_count}_vsp_{timestamp}",
            f"{output_dir}/comt_vsp_details/vsp_{timestamp}",
            f"{output_dir}/comt_vsp_details/{job_num}_tasks_{task_count}_vsp_{timestamp}",
        ]
        
        for vsp_dir in vsp_dirs:
            if os.path.exists(vsp_dir):
                related.append(vsp_dir)
    
    return related


def find_files_to_cleanup(output_dir: str = 'output', threshold: int = 100) -> Dict[str, List[str]]:
    """
    æŸ¥æ‰¾éœ€è¦æ¸…ç†çš„æ–‡ä»¶
    
    Args:
        output_dir: output ç›®å½•è·¯å¾„
        threshold: ä»»åŠ¡æ•°é˜ˆå€¼ï¼Œå°äºæ­¤å€¼çš„æ–‡ä»¶å°†è¢«æ¸…ç†
    
    Returns:
        {jsonl_filename: [related_files_list]}
    """
    cleanup_candidates = {}
    
    # æŸ¥æ‰¾æ‰€æœ‰ JSONL æ–‡ä»¶
    jsonl_files = glob.glob(os.path.join(output_dir, '*.jsonl'))
    
    for jsonl_path in jsonl_files:
        filename = os.path.basename(jsonl_path)
        
        # è·³è¿‡ eval_debug.jsonl æ–‡ä»¶ï¼ˆè¿™äº›ä¼šé€šè¿‡ä¸» JSONL æ–‡ä»¶ä¸€èµ·å¤„ç†ï¼‰
        if filename.endswith('_eval_debug.jsonl'):
            continue
        
        # è§£ææ–‡ä»¶å
        job_num, task_count, rest = parse_jsonl_filename(filename)
        
        if job_num is None or task_count is None:
            # æ— æ³•è§£æçš„æ–‡ä»¶åï¼Œè·³è¿‡
            continue
        
        # æ£€æŸ¥æ˜¯å¦ä½äºé˜ˆå€¼
        if task_count < threshold:
            # æŸ¥æ‰¾æ‰€æœ‰ç›¸å…³æ–‡ä»¶
            related_files = find_related_files(filename, output_dir)
            if related_files:
                cleanup_candidates[filename] = related_files
    
    return cleanup_candidates


def format_file_size(size_bytes: int) -> str:
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if size_bytes < 1024.0:
            return f"{size_bytes:.1f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.1f} TB"


def get_file_or_dir_size(path: str) -> int:
    """è·å–æ–‡ä»¶æˆ–ç›®å½•çš„æ€»å¤§å°"""
    if os.path.isfile(path):
        return os.path.getsize(path)
    elif os.path.isdir(path):
        total = 0
        for dirpath, dirnames, filenames in os.walk(path):
            for filename in filenames:
                filepath = os.path.join(dirpath, filename)
                try:
                    total += os.path.getsize(filepath)
                except OSError:
                    pass
        return total
    return 0


def print_cleanup_summary(cleanup_candidates: Dict[str, List[str]]):
    """æ‰“å°æ¸…ç†æ‘˜è¦"""
    if not cleanup_candidates:
        print("\nâœ… æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„æ–‡ä»¶")
        return
    
    print(f"\n{'='*80}")
    print(f"ğŸ—‘ï¸  æ¸…ç†æ‘˜è¦")
    print(f"{'='*80}\n")
    
    total_files = 0
    total_dirs = 0
    total_size = 0
    
    for i, (jsonl_filename, related_files) in enumerate(sorted(cleanup_candidates.items()), 1):
        # è§£æä¿¡æ¯
        job_num, task_count, rest = parse_jsonl_filename(jsonl_filename)
        
        print(f"{i}. Job {job_num} (tasks={task_count})")
        print(f"   ä¸»æ–‡ä»¶: {jsonl_filename}")
        
        files_count = 0
        dirs_count = 0
        group_size = 0
        
        for related_path in related_files:
            size = get_file_or_dir_size(related_path)
            group_size += size
            
            if os.path.isdir(related_path):
                dirs_count += 1
                total_dirs += 1
                print(f"   â””â”€ [DIR]  {os.path.basename(related_path)} ({format_file_size(size)})")
            else:
                files_count += 1
                total_files += 1
                print(f"   â””â”€ [FILE] {os.path.basename(related_path)} ({format_file_size(size)})")
        
        total_size += group_size
        print(f"   å°è®¡: {files_count} ä¸ªæ–‡ä»¶, {dirs_count} ä¸ªç›®å½•, {format_file_size(group_size)}")
        print()
    
    print(f"{'='*80}")
    print(f"æ€»è®¡: {len(cleanup_candidates)} ä¸ª job, {total_files} ä¸ªæ–‡ä»¶, {total_dirs} ä¸ªç›®å½•")
    print(f"å°†é‡Šæ”¾ç©ºé—´: {format_file_size(total_size)}")
    print(f"{'='*80}\n")


def delete_files_and_dirs(file_list: List[str]) -> Tuple[int, int]:
    """
    åˆ é™¤æ–‡ä»¶å’Œç›®å½•
    
    Returns:
        (deleted_files, deleted_dirs)
    """
    import shutil
    
    deleted_files = 0
    deleted_dirs = 0
    
    for path in file_list:
        try:
            if os.path.isdir(path):
                shutil.rmtree(path)
                deleted_dirs += 1
                print(f"  âœ… å·²åˆ é™¤ç›®å½•: {path}")
            elif os.path.isfile(path):
                os.remove(path)
                deleted_files += 1
                print(f"  âœ… å·²åˆ é™¤æ–‡ä»¶: {path}")
        except Exception as e:
            print(f"  âŒ åˆ é™¤å¤±è´¥ {path}: {e}")
    
    return deleted_files, deleted_dirs


def main():
    parser = argparse.ArgumentParser(
        description="æ¸…ç† output/ ç›®å½•ä¸­ä»»åŠ¡æ•°å°äºé˜ˆå€¼çš„æ–‡ä»¶",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # é¢„è§ˆå°†è¦åˆ é™¤çš„æ–‡ä»¶ï¼ˆä¸å®é™…åˆ é™¤ï¼‰
  python cleanup_output.py --dry-run
  
  # æ¸…ç†ä»»åŠ¡æ•° < 100 çš„æ–‡ä»¶ï¼ˆé»˜è®¤ï¼‰
  python cleanup_output.py
  
  # æ¸…ç†ä»»åŠ¡æ•° < 50 çš„æ–‡ä»¶
  python cleanup_output.py --threshold 50
  
  # è‡ªåŠ¨ç¡®è®¤åˆ é™¤ï¼ˆä¸éœ€è¦äº¤äº’ï¼‰
  python cleanup_output.py --yes
        """
    )
    
    parser.add_argument('--threshold', type=int, default=100,
                       help='ä»»åŠ¡æ•°é˜ˆå€¼ï¼Œå°äºæ­¤å€¼çš„æ–‡ä»¶å°†è¢«æ¸…ç†ï¼ˆé»˜è®¤: 100ï¼‰')
    parser.add_argument('--output_dir', default='output',
                       help='output ç›®å½•è·¯å¾„ï¼ˆé»˜è®¤: outputï¼‰')
    parser.add_argument('--dry-run', action='store_true',
                       help='é¢„è§ˆæ¨¡å¼ï¼šåªæ˜¾ç¤ºå°†è¦åˆ é™¤çš„æ–‡ä»¶ï¼Œä¸å®é™…åˆ é™¤')
    parser.add_argument('--yes', '-y', action='store_true',
                       help='è‡ªåŠ¨ç¡®è®¤ï¼Œä¸éœ€è¦äº¤äº’å¼è¯¢é—®')
    
    args = parser.parse_args()
    
    print(f"{'='*80}")
    print(f"ğŸ§¹ output/ ç›®å½•æ¸…ç†å·¥å…·")
    print(f"{'='*80}")
    print(f"ç›®å½•: {args.output_dir}")
    print(f"é˜ˆå€¼: tasks < {args.threshold}")
    if args.dry_run:
        print(f"æ¨¡å¼: é¢„è§ˆæ¨¡å¼ï¼ˆä¸ä¼šå®é™…åˆ é™¤ï¼‰")
    print(f"{'='*80}\n")
    
    # æŸ¥æ‰¾éœ€è¦æ¸…ç†çš„æ–‡ä»¶
    print("ğŸ” æ‰«ææ–‡ä»¶...")
    cleanup_candidates = find_files_to_cleanup(args.output_dir, args.threshold)
    
    if not cleanup_candidates:
        print("\nâœ… æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„æ–‡ä»¶")
        return
    
    # æ‰“å°æ‘˜è¦
    print_cleanup_summary(cleanup_candidates)
    
    # å¦‚æœæ˜¯é¢„è§ˆæ¨¡å¼ï¼Œç›´æ¥é€€å‡º
    if args.dry_run:
        print("ğŸ’¡ è¿™æ˜¯é¢„è§ˆæ¨¡å¼ï¼Œæ²¡æœ‰åˆ é™¤ä»»ä½•æ–‡ä»¶")
        print("   è¦å®é™…åˆ é™¤ï¼Œè¯·è¿è¡Œ: python cleanup_output.py")
        return
    
    # è¯¢é—®ç”¨æˆ·ç¡®è®¤
    if not args.yes:
        response = input("â“ ç¡®è®¤åˆ é™¤ä»¥ä¸Šæ–‡ä»¶ï¼Ÿ(yes/no): ").strip().lower()
        if response not in ['yes', 'y']:
            print("\nâŒ å–æ¶ˆåˆ é™¤")
            return
    
    # æ‰§è¡Œåˆ é™¤
    print(f"\n{'='*80}")
    print(f"ğŸ—‘ï¸  å¼€å§‹åˆ é™¤...")
    print(f"{'='*80}\n")
    
    total_deleted_files = 0
    total_deleted_dirs = 0
    
    for jsonl_filename, related_files in sorted(cleanup_candidates.items()):
        job_num, task_count, _ = parse_jsonl_filename(jsonl_filename)
        print(f"\nğŸ—‘ï¸  åˆ é™¤ Job {job_num} (tasks={task_count}):")
        
        deleted_files, deleted_dirs = delete_files_and_dirs(related_files)
        total_deleted_files += deleted_files
        total_deleted_dirs += deleted_dirs
    
    # æ‰“å°å®Œæˆæ‘˜è¦
    print(f"\n{'='*80}")
    print(f"âœ… æ¸…ç†å®Œæˆï¼")
    print(f"{'='*80}")
    print(f"å·²åˆ é™¤: {len(cleanup_candidates)} ä¸ª job")
    print(f"  - æ–‡ä»¶: {total_deleted_files} ä¸ª")
    print(f"  - ç›®å½•: {total_deleted_dirs} ä¸ª")
    print(f"{'='*80}\n")


if __name__ == "__main__":
    main()


